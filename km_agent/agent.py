"""
KM Agent - Knowledge Management Agent
Intelligent agent for knowledge base query and management
"""

import json
import sys
import os
from typing import List, Dict, Optional, Any

# Import PDFVectorizer
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from pdf_vectorizer import PDFVectorizer

# Import ks_infrastructure services
from ks_infrastructure import ks_openai, ks_user_info
from ks_infrastructure.configs.default import OPENAI_CONFIG
from ks_infrastructure import get_current_user

# Import agent tools
from km_agent.tools import AgentTools


class KMAgent:
    """
    Knowledge Management Agent

    Features:
    - Query knowledge base using semantic search
    - Retrieve complete knowledge by fetching subsequent pages
    - Provide accurate answers based on knowledge base
    - Never fabricate information
    """

    SYSTEM_PROMPT = """ä½ æ˜¯é‡‘å±±çŸ¥è¯†ç®¡ç†agentï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·éœ€æ±‚æŸ¥è¯¢ç›¸å…³çŸ¥è¯†åº“ï¼Œå¹¶ä¸”æ ¹æ®çŸ¥è¯†åº“è¿”å›çš„å†…å®¹ï¼Œç»™ç”¨æˆ·å‡†ç¡®ç­”æ¡ˆã€‚

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœæŸä¸ªçŸ¥è¯†åˆ‡ç‰‡çš„å†…å®¹ä¸å®Œå…¨ï¼Œä½ å¯ä»¥ç»§ç»­æŸ¥è¯¢å‡ºä»–åç»­çš„åˆ‡ç‰‡ï¼Œä»¥ä¿è¯çŸ¥è¯†å®Œå…¨
2. å¦‚æœç”¨æˆ·æƒ³è¦æ–°å¢çŸ¥è¯†åº“ï¼Œä½ éœ€è¦æé†’ç”¨æˆ·"ç‚¹å‡»è¾“å…¥æ¡†çš„çŸ¥è¯†æ ‡è¯†ï¼Œå¯ä»¥ä¸Šä¼ æ‚¨çš„çŸ¥è¯†"
3. å¦‚æœçŸ¥è¯†åº“æŸ¥è¯¢ä¹‹åçœ‹ä¸åˆ°åŒ¹é…çš„ç­”æ¡ˆï¼Œå‘Šè¯‰ç”¨æˆ·çŸ¥è¯†åº“ç›®å‰è¿˜æ²¡æœ‰æ­¤ç±»ä¿¡æ¯
4. ä½ çš„ç»“æœå¯ä»¥æ˜¯æ²¡æœ‰ä¿¡æ¯ï¼Œä½†æ˜¯ä¸€å®šä¸è¦è‡ªå·±ç¼–é€ ä¿¡æ¯
5. **å›ç­”æ ¼å¼è¦æ±‚**ï¼š
   - ä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡å›ç­”ï¼Œä¸¥ç¦ä½¿ç”¨HTMLæ ‡ç­¾
   - ä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨ã€ç²—ä½“ç­‰æ ¼å¼ä½¿å†…å®¹æ›´æ¸…æ™°
   
6. **å¼•ç”¨æ–‡æ¡£è§„åˆ™ï¼ˆæå…¶é‡è¦ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
   - **åªæœ‰åœ¨search_knowledge æˆ– get_pages å·¥å…·ä¸­è·å¾—äº†ç›®æ ‡çŸ¥è¯†ï¼Œæ‰èƒ½æ·»åŠ "å¼•ç”¨æ–‡æ¡£"éƒ¨åˆ†**
   - **å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç­”æ¡ˆï¼Œç»å¯¹ä¸è¦æ˜¾ç¤º"å¼•ç”¨æ–‡æ¡£"éƒ¨åˆ†**
   - **å¼•ç”¨çš„æ–‡æ¡£åå’Œé¡µç å¿…é¡»å®Œå…¨æ¥è‡ªå·¥å…·è¿”å›çš„ç»“æœï¼Œç¦æ­¢è‡ªå·±ç¼–é€ æˆ–çŒœæµ‹**
   - **å¼•ç”¨æ–‡æ¡£å¿…é¡»ä½¿ç”¨Markdowné“¾æ¥è¯­æ³•ï¼Œä¸èƒ½ä½¿ç”¨HTML <a>æ ‡ç­¾**
   - **å¼•ç”¨æ ¼å¼ï¼ˆä»…åœ¨æœ‰å·¥å…·ç»“æœæ—¶ä½¿ç”¨ï¼‰**ï¼š

     **å¼•ç”¨æ–‡æ¡£ï¼š**
     - ğŸ“„ [å±…ä½è¯åŠç†.pdf:2](http://pdf/å±…ä½è¯åŠç†.pdf:2)
     - ğŸ“„ [å¦ä¸€ä¸ªæ–‡æ¡£.pdf:3](http://pdf/å¦ä¸€ä¸ªæ–‡æ¡£.pdf:3)

   - **Markdowné“¾æ¥æ ¼å¼è§„åˆ™**ï¼š
     * æ˜¾ç¤ºæ–‡æœ¬æ ¼å¼ï¼š`æ–‡æ¡£å.pdf:é¡µç `
     * é“¾æ¥åœ°å€æ ¼å¼ï¼š`http://pdf/æ–‡æ¡£å.pdf:é¡µç `
     * å®Œæ•´ç¤ºä¾‹ï¼š`[å±…ä½è¯åŠç†.pdf:2](http://pdf/å±…ä½è¯åŠç†.pdf:2)` âœ“
     * é”™è¯¯ç¤ºä¾‹ï¼š
       - `[å±…ä½è¯åŠç†.pdf:2](å±…ä½è¯åŠç†.pdf:2)` âœ— é“¾æ¥åœ°å€ç¼ºå°‘ http://pdf/ å‰ç¼€
       - `[å±…ä½è¯åŠç†.pdf:2]()` âœ— é“¾æ¥åœ°å€ä¸ºç©º
       - `<a href="">å±…ä½è¯åŠç†.pdf:2</a>` âœ— ç¦æ­¢ä½¿ç”¨HTML
   - **å…³é”®**ï¼šé“¾æ¥åœ°å€å¿…é¡»æ˜¯ `http://pdf/æ–‡æ¡£å.pdf:é¡µç ` æ ¼å¼ï¼
   - **å†æ¬¡å¼ºè°ƒ**ï¼šæ²¡æœ‰è°ƒç”¨å·¥å…·æˆ–å·¥å…·æ— ç»“æœæ—¶ï¼Œä¸è¦æ˜¾ç¤ºä»»ä½•å¼•ç”¨ï¼

ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥ä½¿ç”¨ï¼š
- search_knowledge: æœç´¢çŸ¥è¯†åº“ï¼Œè¿”å›ç›¸å…³çš„çŸ¥è¯†åˆ‡ç‰‡
- get_pages: æ ¹æ®æ–‡ä»¶åå’Œé¡µç è·å–å®Œæ•´çš„çŸ¥è¯†å†…å®¹
- get_subordinate_attendance: è·å–ä¸‹å±çš„è€ƒå‹¤è®°å½•ï¼ˆéœ€è¦æƒé™éªŒè¯ï¼‰
- get_manager_style: è·å–ç®¡ç†è€…çš„ç®¡ç†é£æ ¼ç±»å‹
- get_current_time: è·å–å½“å‰æ—¶é—´
- get_subordinates: è·å–æŒ‡å®šç”¨æˆ·çš„ä¸‹å±åˆ—è¡¨ï¼ˆä¸æŒ‡å®šåˆ™è·å–å½“å‰ç”¨æˆ·çš„ä¸‹å±ï¼‰
- get_subordinate_employee_info: è·å–ä¸‹å±çš„å‘˜å·¥ä¿¡æ¯ï¼ˆéœ€è¦æƒé™éªŒè¯ï¼‰
"""

    def __init__(
        self,
        verbose: bool = False,
        owner: str = None,
        conversation_id: str = None,
        enable_history: bool = False
    ):
        """
        Initialize KM Agent

        Args:
            verbose: Whether to print debug information
            owner: User identifier for loading custom instructions
            conversation_id: Conversation ID for history persistence (optional)
            enable_history: Whether to enable conversation history persistence

        Note:
            All configuration (OpenAI, embedding, Qdrant) is automatically loaded
            from ks_infrastructure services. No need to pass any parameters.
        """
        self.verbose = verbose
        self.owner = owner if owner else get_current_user()

        # LLM client (using ks_infrastructure)
        self.llm_client = ks_openai()
        self.llm_model = OPENAI_CONFIG.get("model", "DeepSeek-V3.1-Ksyun")

        # Vectorizer for knowledge base operations (using ks_infrastructure)
        # collection_name, vector_size are defaults in PDFVectorizer
        self.vectorizer = PDFVectorizer()

        # User info service for HR operations
        self.user_info_service = ks_user_info()

        # Initialize agent tools
        self.agent_tools = AgentTools(
            vectorizer=self.vectorizer,
            user_info_service=self.user_info_service,
            verbose=self.verbose
        )

        # Get tool definitions from agent tools
        self.tools = self.agent_tools.get_tool_definitions()

        # Load user custom instructions
        self.custom_instructions = self._load_instructions()

        # Build effective system prompt with custom instructions
        self.effective_system_prompt = self._build_system_prompt()
        
        # Conversation manager (optional)
        self.conversation_manager = None
        if enable_history:
            from km_agent.conversation_manager import ConversationManager
            self.conversation_manager = ConversationManager(
                owner=self.owner,
                conversation_id=conversation_id,
                verbose=self.verbose
            )
            # If no conversation_id provided, create a new conversation
            if not conversation_id:
                self.conversation_manager.start_conversation()
            
            if self.verbose:
                print(f"[ConversationManager] Enabled with conversation_id: {self.conversation_manager.get_conversation_id()}")

    def _load_instructions(self) -> list:
        """
        Load user's custom instructions from database
        
        Returns:
            List of active instructions ordered by priority
        """
        try:
            from instruction_repository import get_active_instructions
            return get_active_instructions(self.owner)
        except Exception as e:
            if self.verbose:
                print(f"Warning: Failed to load custom instructions: {e}")
            return []
    
    def _build_system_prompt(self) -> str:
        """
        Build effective system prompt by combining base prompt with custom instructions
        
        Returns:
            Complete system prompt string
        """
        base_prompt = self.SYSTEM_PROMPT
        
        if not self.custom_instructions:
            return base_prompt
        
        # Format custom instructions
        instructions_text = "\n".join([
            f"{i+1}. {inst['content']}" 
            for i, inst in enumerate(self.custom_instructions)
        ])
        
        # Append custom instructions to base prompt
        return f"""{base_prompt}

**ç”¨æˆ·è‡ªå®šä¹‰æŒ‡ç¤º**ï¼ˆè¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è¦æ±‚ï¼‰ï¼š
{instructions_text}
"""
    
    def reload_instructions(self):
        """
        Reload custom instructions from database
        
        Useful when instructions are updated during agent lifecycle
        """
        self.custom_instructions = self._load_instructions()
        self.effective_system_prompt = self._build_system_prompt()
        if self.verbose:
            print(f"Reloaded {len(self.custom_instructions)} custom instructions")

    def _execute_tool(self, tool_name: str, tool_args: Dict[str, Any]) -> str:
        """
        Execute a tool function

        Args:
            tool_name: Name of the tool to execute
            tool_args: Arguments for the tool

        Returns:
            JSON string of tool result
        """
        return self.agent_tools.execute_tool(tool_name, tool_args, current_user=self.owner)

    def chat_stream(self, user_message: str, history: Optional[List[Dict]] = None):
        """
        Chat with the agent using streaming response

        Args:
            user_message: User's message
            history: Optional conversation history

        Yields:
            Dictionary chunks containing:
            - type: 'tool_call' | 'content' | 'done'
            - data: Tool call info or content chunk
        """
        if history is None:
            history = []

        # Always use the latest system prompt (important for dynamic instruction updates)
        if not history:
            messages = [{"role": "system", "content": self.effective_system_prompt}]
            # Save system message if conversation manager is enabled
            if self.conversation_manager:
                self.conversation_manager.save_system_message(self.effective_system_prompt)
        else:
            # Copy history but update the system prompt to reflect latest instructions
            messages = history.copy()
            # Replace the first system message with the updated prompt
            if messages and messages[0]["role"] == "system":
                messages[0] = {"role": "system", "content": self.effective_system_prompt}
                if self.verbose:
                    print(f"\n[DEBUG] Updated system prompt in history")
                    print(f"[DEBUG] Custom instructions count: {len(self.custom_instructions)}")

        # Add user message
        messages.append({"role": "user", "content": user_message})
        
        # Save user message to database
        if self.conversation_manager:
            self.conversation_manager.save_user_message(user_message)

        tool_calls_made = []
        max_iterations = 5
        iteration = 0

        while iteration < max_iterations:
            iteration += 1

            if self.verbose:
                print(f"\n[Iteration {iteration}] Calling LLM with streaming...")
                # Debug: Print full prompt
                print("-" * 20 + " Full Prompt " + "-" * 20)
                print(json.dumps(messages, ensure_ascii=False, indent=2))
                print("-" * 50)

            # Call LLM with streaming
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                tools=self.tools,
                tool_choice="auto",
                stream=True
            )

            # Collect streaming response
            collected_content = ""
            collected_tool_calls = []
            current_tool_call = None

            for chunk in response:
                # Check if choices is empty (final chunk in stream)
                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta

                # Handle content streaming
                if delta.content:
                    collected_content += delta.content
                    # Yield content chunk immediately for streaming experience
                    yield {
                        "type": "content",
                        "data": delta.content
                    }

                # Handle tool calls
                if delta.tool_calls:
                    for tc_chunk in delta.tool_calls:
                        if tc_chunk.index is not None:
                            # New tool call
                            while len(collected_tool_calls) <= tc_chunk.index:
                                collected_tool_calls.append({
                                    "id": "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                })
                            current_tool_call = collected_tool_calls[tc_chunk.index]

                        if tc_chunk.id:
                            current_tool_call["id"] = tc_chunk.id
                        if tc_chunk.function:
                            if tc_chunk.function.name:
                                current_tool_call["function"]["name"] = tc_chunk.function.name
                            if tc_chunk.function.arguments:
                                current_tool_call["function"]["arguments"] += tc_chunk.function.arguments

            # Add assistant message to history (in-memory)
            messages.append({
                "role": "assistant",
                "content": collected_content,
                "tool_calls": collected_tool_calls if collected_tool_calls else None
            })
            
            # Only save assistant message to database if it's the final answer (no tool calls)
            if not collected_tool_calls:
                if self.conversation_manager:
                    self.conversation_manager.save_assistant_message(
                        content=collected_content,
                        tool_calls=None
                    )
                
                # Get clean history (consistent with DB) for the frontend
                # Filter out intermediate tool calls and tool results from the returned history
                clean_history = []
                for msg in messages:
                    # Keep user and system messages
                    if msg['role'] in ['user', 'system']:
                        clean_history.append(msg)
                    # Keep assistant messages ONLY if they don't have tool_calls
                    elif msg['role'] == 'assistant' and not msg.get('tool_calls'):
                        clean_history.append(msg)
                    # Skip tool messages (role='tool')
                
                yield {
                    "type": "done",
                    "data": {
                        "tool_calls": tool_calls_made,
                        "history": clean_history,
                        "conversation_id": self.conversation_manager.get_conversation_id() if self.conversation_manager else None
                    }
                }
                break

            # Execute tool calls
            for tool_call in collected_tool_calls:
                tool_name = tool_call["function"]["name"]
                tool_args = json.loads(tool_call["function"]["arguments"])

                if self.verbose:
                    print(f"[Iteration {iteration}] Executing tool: {tool_name}")

                # Yield tool call notification (optional, frontend logs it but doesn't show in chat bubble)
                yield {
                    "type": "tool_call",
                    "data": {
                        "tool": tool_name,
                        "arguments": tool_args
                    }
                }

                # Execute tool
                tool_result = self._execute_tool(tool_name, tool_args)

                # Record tool call
                tool_calls_made.append({
                    "tool": tool_name,
                    "arguments": tool_args,
                    "result": json.loads(tool_result)
                })

                # Add tool result to messages (in-memory)
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": tool_result
                })
                
                # Do NOT save tool message to database (as per user requirement)

    def run(self):
        """
        Run interactive chat session using streaming
        """
        print("="*80)
        print("é‡‘å±±çŸ¥è¯†ç®¡ç† Agent")
        print("="*80)
        print("æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("="*80)

        history = None

        while True:
            try:
                user_input = input("\nç”¨æˆ·: ").strip()

                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("\nå†è§ï¼")
                    break

                if not user_input:
                    continue

                print("\nAgent: ", end="", flush=True)

                # Use chat_stream instead of chat
                tool_calls_made = []
                final_history = None

                for chunk in self.chat_stream(user_input, history):
                    if chunk["type"] == "content":
                        print(chunk["data"], end="", flush=True)
                    elif chunk["type"] == "tool_call":
                        tool_calls_made.append(chunk["data"])
                    elif chunk["type"] == "done":
                        final_history = chunk["data"]["history"]
                        tool_calls_made = chunk["data"]["tool_calls"]

                print()  # New line after response
                history = final_history

                if self.verbose and tool_calls_made:
                    print(f"\n[Debug] Tool calls: {len(tool_calls_made)}")
                    for i, tc in enumerate(tool_calls_made, 1):
                        print(f"  {i}. {tc['tool']}: {tc['arguments']}")

            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\né”™è¯¯: {e}")
                if self.verbose:
                    import traceback
                    traceback.print_exc()
