"""
KM Agent - Knowledge Management Agent
Intelligent agent for knowledge base query and management
"""

import json
import sys
import os
from typing import List, Dict, Optional, Any

# Import PDFVectorizer
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from pdf_vectorizer import PDFVectorizer

# Import ks_infrastructure services
from ks_infrastructure import ks_openai
from ks_infrastructure.configs.default import OPENAI_CONFIG


class KMAgent:
    """
    Knowledge Management Agent

    Features:
    - Query knowledge base using semantic search
    - Retrieve complete knowledge by fetching subsequent pages
    - Provide accurate answers based on knowledge base
    - Never fabricate information
    """

    SYSTEM_PROMPT = """ä½ æ˜¯é‡‘å±±çŸ¥è¯†ç®¡ç†agentï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·éœ€æ±‚æŸ¥è¯¢ç›¸å…³çŸ¥è¯†åº“ï¼Œå¹¶ä¸”æ ¹æ®çŸ¥è¯†åº“è¿”å›çš„å†…å®¹ï¼Œç»™ç”¨æˆ·å‡†ç¡®ç­”æ¡ˆã€‚

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœæŸä¸ªçŸ¥è¯†åˆ‡ç‰‡çš„å†…å®¹ä¸å®Œå…¨ï¼Œä½ å¯ä»¥ç»§ç»­æŸ¥è¯¢å‡ºä»–åç»­çš„åˆ‡ç‰‡ï¼Œä»¥ä¿è¯çŸ¥è¯†å®Œå…¨
2. å¦‚æœç”¨æˆ·æƒ³è¦æ–°å¢çŸ¥è¯†åº“ï¼Œä½ éœ€è¦æé†’ç”¨æˆ·"ç‚¹å‡»è¾“å…¥æ¡†çš„çŸ¥è¯†æ ‡è¯†ï¼Œå¯ä»¥ä¸Šä¼ æ‚¨çš„çŸ¥è¯†"
3. å¦‚æœçŸ¥è¯†åº“æŸ¥è¯¢ä¹‹åçœ‹ä¸åˆ°åŒ¹é…çš„ç­”æ¡ˆï¼Œå‘Šè¯‰ç”¨æˆ·çŸ¥è¯†åº“ç›®å‰è¿˜æ²¡æœ‰æ­¤ç±»ä¿¡æ¯
4. ä½ çš„ç»“æœå¯ä»¥æ˜¯æ²¡æœ‰ä¿¡æ¯ï¼Œä½†æ˜¯ä¸€å®šä¸è¦è‡ªå·±ç¼–é€ ä¿¡æ¯
5. **å›ç­”æ ¼å¼è¦æ±‚**ï¼š
   - ä½¿ç”¨ Markdown æ ¼å¼ç»„ç»‡å›ç­”ï¼Œä¸¥ç¦ä½¿ç”¨HTMLæ ‡ç­¾
   - ä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨ã€ç²—ä½“ç­‰æ ¼å¼ä½¿å†…å®¹æ›´æ¸…æ™°
   - åœ¨å›ç­”çš„æœ€åï¼Œå¿…é¡»æ·»åŠ "å¼•ç”¨æ–‡æ¡£"éƒ¨åˆ†
   - **é‡è¦ï¼šå¼•ç”¨æ–‡æ¡£å¿…é¡»ä½¿ç”¨Markdowné“¾æ¥è¯­æ³•ï¼Œä¸èƒ½ä½¿ç”¨HTML <a>æ ‡ç­¾**
   - **å¼•ç”¨æ ¼å¼ï¼ˆè¯·ä¸¥æ ¼éµå®ˆï¼Œç›´æ¥ä½¿ç”¨æ­¤æ ¼å¼ï¼‰**ï¼š

     **å¼•ç”¨æ–‡æ¡£ï¼š**
     - ğŸ“„ [å±…ä½è¯åŠç†.pdf:2](http://pdf/å±…ä½è¯åŠç†.pdf:2)
     - ğŸ“„ [å¦ä¸€ä¸ªæ–‡æ¡£.pdf:3](http://pdf/å¦ä¸€ä¸ªæ–‡æ¡£.pdf:3)

   - **Markdowné“¾æ¥æ ¼å¼è§„åˆ™**ï¼š
     * æ˜¾ç¤ºæ–‡æœ¬æ ¼å¼ï¼š`æ–‡æ¡£å.pdf:é¡µç `
     * é“¾æ¥åœ°å€æ ¼å¼ï¼š`http://pdf/æ–‡æ¡£å.pdf:é¡µç `
     * å®Œæ•´ç¤ºä¾‹ï¼š`[å±…ä½è¯åŠç†.pdf:2](http://pdf/å±…ä½è¯åŠç†.pdf:2)` âœ“
     * é”™è¯¯ç¤ºä¾‹ï¼š
       - `[å±…ä½è¯åŠç†.pdf:2](å±…ä½è¯åŠç†.pdf:2)` âœ— é“¾æ¥åœ°å€ç¼ºå°‘ http://pdf/ å‰ç¼€
       - `[å±…ä½è¯åŠç†.pdf:2]()` âœ— é“¾æ¥åœ°å€ä¸ºç©º
       - `<a href="">å±…ä½è¯åŠç†.pdf:2</a>` âœ— ç¦æ­¢ä½¿ç”¨HTML
   - **å…³é”®**ï¼šé“¾æ¥åœ°å€å¿…é¡»æ˜¯ `http://pdf/æ–‡æ¡£å.pdf:é¡µç ` æ ¼å¼ï¼

ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥ä½¿ç”¨ï¼š
- search_knowledge: æœç´¢çŸ¥è¯†åº“ï¼Œè¿”å›ç›¸å…³çš„çŸ¥è¯†åˆ‡ç‰‡
- get_pages: æ ¹æ®æ–‡ä»¶åå’Œé¡µç è·å–å®Œæ•´çš„çŸ¥è¯†å†…å®¹
"""

    def __init__(
        self,
        verbose: bool = False
    ):
        """
        Initialize KM Agent

        Args:
            verbose: Whether to print debug information

        Note:
            All configuration (OpenAI, embedding, Qdrant) is automatically loaded
            from ks_infrastructure services. No need to pass any parameters.
        """
        self.verbose = verbose

        # LLM client (using ks_infrastructure)
        self.llm_client = ks_openai()
        self.llm_model = OPENAI_CONFIG.get("model", "DeepSeek-V3.1-Ksyun")

        # Vectorizer for knowledge base operations (using ks_infrastructure)
        # collection_name, vector_size are defaults in PDFVectorizer
        self.vectorizer = PDFVectorizer()

        # Tool definitions for function calling
        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "search_knowledge",
                    "description": "æœç´¢çŸ¥è¯†åº“ï¼Œè¿”å›ä¸æŸ¥è¯¢ç›¸å…³çš„çŸ¥è¯†åˆ‡ç‰‡ã€‚ä½¿ç”¨è¯­ä¹‰æœç´¢æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£é¡µé¢ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æœç´¢æŸ¥è¯¢å†…å®¹"
                            },
                            "limit": {
                                "type": "integer",
                                "description": "è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5",
                                "default": 5
                            },
                            "mode": {
                                "type": "string",
                                "enum": ["content"],
                                "description": "æœç´¢æ¨¡å¼ï¼šåªä½¿ç”¨content(å†…å®¹)å¬å›",
                                "default": "content"
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_pages",
                    "description": "æ ¹æ®æ–‡ä»¶åå’Œé¡µç è·å–å®Œæ•´çš„çŸ¥è¯†å†…å®¹ã€‚å½“éœ€è¦æŸ¥çœ‹å®Œæ•´å†…å®¹æˆ–åç»­é¡µé¢æ—¶ä½¿ç”¨ã€‚",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "filename": {
                                "type": "string",
                                "description": "æ–‡æ¡£æ–‡ä»¶å"
                            },
                            "page_numbers": {
                                "type": "array",
                                "items": {"type": "integer"},
                                "description": "è¦è·å–çš„é¡µç åˆ—è¡¨ï¼Œä¾‹å¦‚ [1, 2, 3]"
                            },
                            "fields": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "è¦è¿”å›çš„å­—æ®µåˆ—è¡¨ï¼Œå¯é€‰å€¼ï¼šfilename, page_number, content, ownerã€‚é»˜è®¤è¿”å› filename, page_number, contentã€‚"
                            }
                        },
                        "required": ["filename", "page_numbers"]
                    }
                }
            }
        ]

    def _search_knowledge(self, query: str, limit: int = 5, mode: str = "content") -> Dict:
        """
        Search knowledge base

        Args:
            query: Search query
            limit: Number of results
            mode: Search mode (dual/summary/content)

        Returns:
            Search results
        """
        if self.verbose:
            print(f"\n[Tool] search_knowledge: query='{query}', limit={limit}, mode={mode}")

        results = self.vectorizer.search(query, limit=limit, mode=mode, verbose=False)

        # Format results for LLM - only return content, no summary
        formatted_results = []

        if "content_results" in results:
            for result in results["content_results"]:
                formatted_results.append({
                    "filename": result["filename"],
                    "page_number": result["page_number"],
                    "score": result["score"],
                    "content": result["content"]  # Return full content, not preview
                })

        if self.verbose:
            print(f"[Tool] Found {len(formatted_results)} results")

        return {
            "success": True,
            "total_results": len(formatted_results),
            "results": formatted_results
        }

    def _get_pages(self, filename: str, page_numbers: List[int], fields: Optional[List[str]] = None) -> Dict:
        """
        Get specific pages from knowledge base

        Args:
            filename: Document filename
            page_numbers: List of page numbers to retrieve
            fields: Optional list of fields to return (default: filename, page_number, content)

        Returns:
            Page data
        """
        # Default fields: only content-related fields, no summary
        if fields is None:
            fields = ["filename", "page_number", "content"]

        if self.verbose:
            print(f"\n[Tool] get_pages: filename='{filename}', page_numbers={page_numbers}, fields={fields}")

        pages = self.vectorizer.get_pages(
            filename=filename,
            page_numbers=page_numbers,
            fields=fields,
            verbose=False
        )

        if self.verbose:
            print(f"[Tool] Retrieved {len(pages)} pages")

        return {
            "success": True,
            "total_pages": len(pages),
            "pages": pages
        }

    def _execute_tool(self, tool_name: str, tool_args: Dict[str, Any]) -> str:
        """
        Execute a tool function

        Args:
            tool_name: Name of the tool to execute
            tool_args: Arguments for the tool

        Returns:
            JSON string of tool result
        """
        try:
            if tool_name == "search_knowledge":
                result = self._search_knowledge(**tool_args)
            elif tool_name == "get_pages":
                result = self._get_pages(**tool_args)
            else:
                result = {"success": False, "error": f"Unknown tool: {tool_name}"}

            return json.dumps(result, ensure_ascii=False)

        except Exception as e:
            error_result = {"success": False, "error": str(e)}
            return json.dumps(error_result, ensure_ascii=False)

    def chat(self, user_message: str, history: Optional[List[Dict]] = None) -> Dict:
        """
        Chat with the agent

        Args:
            user_message: User's message
            history: Optional conversation history

        Returns:
            Dictionary containing:
            - response: Agent's response text
            - tool_calls: List of tool calls made (if any)
            - history: Updated conversation history
        """
        if history is None:
            history = []

        # Add system prompt if this is the first message
        if not history:
            messages = [{"role": "system", "content": self.SYSTEM_PROMPT}]
        else:
            messages = history.copy()

        # Add user message
        messages.append({"role": "user", "content": user_message})

        tool_calls_made = []
        max_iterations = 5  # Prevent infinite loops
        iteration = 0

        while iteration < max_iterations:
            iteration += 1

            if self.verbose:
                print(f"\n[Iteration {iteration}] Calling LLM...")

            # Call LLM
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                tools=self.tools,
                tool_choice="auto"
            )

            assistant_message = response.choices[0].message

            # Add assistant message to history
            messages.append({
                "role": "assistant",
                "content": assistant_message.content,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": tc.type,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    } for tc in (assistant_message.tool_calls or [])
                ] if assistant_message.tool_calls else None
            })

            # If no tool calls, we're done
            if not assistant_message.tool_calls:
                if self.verbose:
                    print(f"[Iteration {iteration}] No tool calls, finishing")
                break

            # Execute tool calls
            for tool_call in assistant_message.tool_calls:
                tool_name = tool_call.function.name
                tool_args = json.loads(tool_call.function.arguments)

                if self.verbose:
                    print(f"[Iteration {iteration}] Executing tool: {tool_name}")

                # Execute tool
                tool_result = self._execute_tool(tool_name, tool_args)

                # Record tool call
                tool_calls_made.append({
                    "tool": tool_name,
                    "arguments": tool_args,
                    "result": json.loads(tool_result)
                })

                # Add tool result to messages
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": tool_result
                })

        # Get final response
        final_response = messages[-1]["content"] if messages[-1]["role"] == "assistant" else "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"

        return {
            "response": final_response,
            "tool_calls": tool_calls_made,
            "history": messages
        }

    def chat_stream(self, user_message: str, history: Optional[List[Dict]] = None):
        """
        Chat with the agent using streaming response

        Args:
            user_message: User's message
            history: Optional conversation history

        Yields:
            Dictionary chunks containing:
            - type: 'tool_call' | 'content' | 'done'
            - data: Tool call info or content chunk
        """
        if history is None:
            history = []

        # Add system prompt if this is the first message
        if not history:
            messages = [{"role": "system", "content": self.SYSTEM_PROMPT}]
        else:
            messages = history.copy()

        # Add user message
        messages.append({"role": "user", "content": user_message})

        tool_calls_made = []
        max_iterations = 5
        iteration = 0

        while iteration < max_iterations:
            iteration += 1

            if self.verbose:
                print(f"\n[Iteration {iteration}] Calling LLM with streaming...")

            # Call LLM with streaming
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                tools=self.tools,
                tool_choice="auto",
                stream=True
            )

            # Collect streaming response
            collected_content = ""
            collected_tool_calls = []
            current_tool_call = None

            for chunk in response:
                # Check if choices is empty (final chunk in stream)
                if not chunk.choices:
                    continue

                delta = chunk.choices[0].delta

                # Handle content streaming
                if delta.content:
                    collected_content += delta.content
                    # Yield content chunk
                    yield {
                        "type": "content",
                        "data": delta.content
                    }

                # Handle tool calls
                if delta.tool_calls:
                    for tc_chunk in delta.tool_calls:
                        if tc_chunk.index is not None:
                            # New tool call
                            while len(collected_tool_calls) <= tc_chunk.index:
                                collected_tool_calls.append({
                                    "id": "",
                                    "type": "function",
                                    "function": {"name": "", "arguments": ""}
                                })
                            current_tool_call = collected_tool_calls[tc_chunk.index]

                        if tc_chunk.id:
                            current_tool_call["id"] = tc_chunk.id
                        if tc_chunk.function:
                            if tc_chunk.function.name:
                                current_tool_call["function"]["name"] = tc_chunk.function.name
                            if tc_chunk.function.arguments:
                                current_tool_call["function"]["arguments"] += tc_chunk.function.arguments

            # Add assistant message to history
            messages.append({
                "role": "assistant",
                "content": collected_content,
                "tool_calls": collected_tool_calls if collected_tool_calls else None
            })

            # If no tool calls, we're done
            if not collected_tool_calls:
                yield {
                    "type": "done",
                    "data": {
                        "tool_calls": tool_calls_made,
                        "history": messages
                    }
                }
                break

            # Execute tool calls
            for tool_call in collected_tool_calls:
                tool_name = tool_call["function"]["name"]
                tool_args = json.loads(tool_call["function"]["arguments"])

                if self.verbose:
                    print(f"[Iteration {iteration}] Executing tool: {tool_name}")

                # Yield tool call notification
                yield {
                    "type": "tool_call",
                    "data": {
                        "tool": tool_name,
                        "arguments": tool_args
                    }
                }

                # Execute tool
                tool_result = self._execute_tool(tool_name, tool_args)

                # Record tool call
                tool_calls_made.append({
                    "tool": tool_name,
                    "arguments": tool_args,
                    "result": json.loads(tool_result)
                })

                # Add tool result to messages
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": tool_result
                })

    def run(self):
        """
        Run interactive chat session
        """
        print("="*80)
        print("é‡‘å±±çŸ¥è¯†ç®¡ç† Agent")
        print("="*80)
        print("æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("="*80)

        history = None

        while True:
            try:
                user_input = input("\nç”¨æˆ·: ").strip()

                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("\nå†è§ï¼")
                    break

                if not user_input:
                    continue

                print("\nAgent: ", end="", flush=True)

                result = self.chat(user_input, history)
                print(result["response"])

                history = result["history"]

                if self.verbose and result["tool_calls"]:
                    print(f"\n[Debug] Tool calls: {len(result['tool_calls'])}")
                    for i, tc in enumerate(result["tool_calls"], 1):
                        print(f"  {i}. {tc['tool']}: {tc['arguments']}")

            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\né”™è¯¯: {e}")
                if self.verbose:
                    import traceback
                    traceback.print_exc()
